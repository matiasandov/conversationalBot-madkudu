{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational bot for Madkudu support using RAG\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation, a technique where the capabilities of a large language model (LLM) are augmented by retrieving information from other systems and inserting them into the LLM’s context window via a prompt. \n",
    "\n",
    "It’s cheaper than fine tunning: fine-tuning models is expensive because the weights of the model parameters themselves must be adjusted. RAG is simply a series of vector/SQL queries and API calls, which cost tiny fractions of a cent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Importing open ai key and the chat gpt model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OPENAI_API_KEY\"] = \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    #chose a cheap model to save credits\n",
    "    model = \"gpt-3.5-turbo-0125\",\n",
    "    #the closer to 0, the more factual the results are going to be\n",
    "    temperature = 0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='telephone', response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 13, 'total_tokens': 14}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_3bc1b5746c', 'finish_reason': 'stop', 'logprobs': None})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"give me one synonym of phone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading the madkudu support data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "#usinf uft 8 since it has special characters\n",
    "loader = TextLoader('all.txt', encoding='utf-8')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Splitting the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#Alternative option\n",
    "#text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    #i chose 500 because the data is not that numerous\n",
    "    # number of characters\n",
    "    chunk_size = 500, \n",
    "    #How many chars are going to be overlapped in between each chunk\n",
    "    chunk_overlap = 250,\n",
    "    \n",
    "    length_function = len,\n",
    "    \n",
    "    #includes index of character where the separation between each chunk is done\n",
    "    add_start_index = True\n",
    "    \n",
    "    )\n",
    "\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Static enrichment: MadKudu partners with Clearbit, HG data, and PredictLeads to enrich your data. Are you already purchasing through ZoomInfo, Bombora, Datafox? No problem, MadKudu can also use this data. \\nCRM information: actually any data from your CRM (Leads, Contacts, Accounts, Opportunities), be it provided by a 3rd party or from your Marketing and Sales input, can be pulled to MadKudu's platform.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[15].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Using hugging face to embed the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (2.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.39.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (1.2.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (1.10.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: Pillow in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.11.0->sentence-transformers) (2.11.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\matts\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Installing collected packages: safetensors\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.2\n",
      "    Uninstalling safetensors-0.3.2:\n",
      "      Successfully uninstalled safetensors-0.3.2\n",
      "Successfully installed safetensors-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Store chunks in FAISS and embed them - Applying RAG\n",
    "\n",
    "Here we are first creating our database with FAISS\n",
    "\n",
    "RAG works better than fine-tuning the model because:\n",
    "\n",
    "It’s cheaper: fine-tuning models is expensive because the weights of the model parameters themselves must be adjusted. RAG is simply a series of vector/SQL queries and API calls, which cost tiny fractions of a cent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create a chain (pipeline) for inferences\n",
    "\n",
    "It allows you to process information in sequences. Could be use to create a a pipeline of \n",
    "- prompts\n",
    "- models \n",
    "- memory buffers\n",
    "\n",
    "Which makes it very powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Create a **document chain** to pass documents \n",
    "This chain will allows us to pass documents to our final Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "#creating a chain on its simplest form\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2 Add the retriever to a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is likelyhood to buy',\n",
       " 'context': [Document(page_content='Likelihood to Buy: refers to the output of a behavioral model predicting the level of engagement of a person or account. \\nLift (Conversion lift): impact on conversion compared to the average population. \\nA positive lift shows a positive impact on conversion of having this trait, or performing this action \\nA negative life shows a negative impact on conversion of having this trait or performing this action', metadata={'source': 'all.txt', 'start_index': 25988}),\n",
       "  Document(page_content='Likelihood to Buy: Looks at any interaction with your website, product, marketing team, sales team to identify patterns of your Leads and Accounts behaviors that lead to conversions. (What has this lead been doing recently?)\\nLead Grade: Combines the Customer Fit and Likelihood to Buy into one score to help you simplify your workflows\\nWe configure these models independently so if you’re not sure which ones are enabled in your account, reach out to your Customer Success Manager.\\n  F.A.Q', metadata={'source': 'all.txt', 'start_index': 6093}),\n",
       "  Document(page_content='the Likelihood to Buy model (Behavioral)\\nhelps answers \"how are they using our product?\", \"what actions are most correlated to conversion?\", \"is now a good time to reach out?\"\\nThe third category of prediction models, the Lead Grade, is simply a combination of the two others.\\nThe Customer Fit model (demographic)', metadata={'source': 'all.txt', 'start_index': 15612}),\n",
       "  Document(page_content='give your sellers playbooks that work, based on 1st party and 3rd party signals \\nprovider your sellers with the right context to engage with prospects effectively \\nHow it works\\nPlug in your data tech stack.\\nInstall the MadKudu package in Salesforce and download the Chrome extension\\nTweak your mappings and scoring models \\nEnable your sellers\\n  MadKudu ingests multiple sources of data to help you qualify your prospects and arm your sellers to engage with them', metadata={'source': 'all.txt', 'start_index': 3489})],\n",
       " 'answer': 'Likelihood to Buy refers to the output of a behavioral model predicting the level of engagement of a person or account based on their interactions with a website, product, marketing team, and sales team. It looks at patterns of leads and accounts behaviors that lead to conversions and helps answer questions such as \"how are they using our product?\", \"what actions are most correlated to conversion?\", and \"is now a good time to reach out?\"'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"what is likelyhood to buy\"\n",
    "})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Creating a memory chain - Adding conversational memory to our model\n",
    "\n",
    "1. First we need to a new create a conversational retrieval chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    \n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faking a conversation to create conversational memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [\n",
    "    HumanMessage(content=\"Can i found here what likelihood to buy is?\"),\n",
    "    AIMessage(content=\"Yes!\")\n",
    "]\n",
    "answer = retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me more about it!\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Likelihood to Buy: refers to the output of a behavioral model predicting the level of engagement of a person or account. \\nLift (Conversion lift): impact on conversion compared to the average population. \\nA positive lift shows a positive impact on conversion of having this trait, or performing this action \\nA negative life shows a negative impact on conversion of having this trait or performing this action', metadata={'source': 'all.txt', 'start_index': 25988}),\n",
       " Document(page_content='the Likelihood to Buy model (Behavioral)\\nhelps answers \"how are they using our product?\", \"what actions are most correlated to conversion?\", \"is now a good time to reach out?\"\\nThe third category of prediction models, the Lead Grade, is simply a combination of the two others.\\nThe Customer Fit model (demographic)', metadata={'source': 'all.txt', 'start_index': 15612}),\n",
       " Document(page_content='Likelihood to Buy: Looks at any interaction with your website, product, marketing team, sales team to identify patterns of your Leads and Accounts behaviors that lead to conversions. (What has this lead been doing recently?)\\nLead Grade: Combines the Customer Fit and Likelihood to Buy into one score to help you simplify your workflows\\nWe configure these models independently so if you’re not sure which ones are enabled in your account, reach out to your Customer Success Manager.\\n  F.A.Q', metadata={'source': 'all.txt', 'start_index': 6093}),\n",
       " Document(page_content='Customer Fit: Looks at firmographics, demographics and technographic data to identify who from your Leads and Accounts are most likely to convert (Who is this person? Is the company they work for a good fit?)\\nLikelihood to Buy: Looks at any interaction with your website, product, marketing team, sales team to identify patterns of your Leads and Accounts behaviors that lead to conversions. (What has this lead been doing recently?)', metadata={'source': 'all.txt', 'start_index': 5884})]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Scaling the chain to store all the conversations with ** Document chain **\n",
    "6.2.1 Add a document chain that will be able to store the history of the conversation in MessagePlaceholder objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "#prompt containing the past history\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    # this is the variable that should be received where the conversation is going to be stored\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "#this will store the conversation\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "\n",
    "\n",
    "conversational_retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.2.2 Test it with a new history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    HumanMessage(content=\"Can you tell me what likelihood to buy is?\"),\n",
    "    AIMessage(content=\"Likelihood to Buy: refers to the output of a behavioral model predicting the level of engagement of a person or account \")\n",
    "]\n",
    "\n",
    "response = conversational_retrieval_chain.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    \"input\": \"What model do you use\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can you tell me what likelihood to buy is?'),\n",
       "  AIMessage(content='Likelihood to Buy: refers to the output of a behavioral model predicting the level of engagement of a person or account ')],\n",
       " 'input': 'What mode! do you use',\n",
       " 'context': [Document(page_content='the Likelihood to Buy model (Behavioral)\\nhelps answers \"how are they using our product?\", \"what actions are most correlated to conversion?\", \"is now a good time to reach out?\"\\nThe third category of prediction models, the Lead Grade, is simply a combination of the two others.\\nThe Customer Fit model (demographic)', metadata={'source': 'all.txt', 'start_index': 15612}),\n",
       "  Document(page_content='Likelihood to Buy: refers to the output of a behavioral model predicting the level of engagement of a person or account. \\nLift (Conversion lift): impact on conversion compared to the average population. \\nA positive lift shows a positive impact on conversion of having this trait, or performing this action \\nA negative life shows a negative impact on conversion of having this trait or performing this action', metadata={'source': 'all.txt', 'start_index': 25988}),\n",
       "  Document(page_content='Likelihood to Buy: Looks at any interaction with your website, product, marketing team, sales team to identify patterns of your Leads and Accounts behaviors that lead to conversions. (What has this lead been doing recently?)\\nLead Grade: Combines the Customer Fit and Likelihood to Buy into one score to help you simplify your workflows\\nWe configure these models independently so if you’re not sure which ones are enabled in your account, reach out to your Customer Success Manager.\\n  F.A.Q', metadata={'source': 'all.txt', 'start_index': 6093}),\n",
       "  Document(page_content='Intent data: information collected about visitor behavior on 3rd party website showing a level of interest in a solution. Examples of providers: G2Crowd, Bombora... \\nLead Grade: model combining the Customer Fit and Likelihood to Buy\\nLikelihood to Buy: refers to the output of a behavioral model predicting the level of engagement of a person or account. \\nLift (Conversion lift): impact on conversion compared to the average population.', metadata={'source': 'all.txt', 'start_index': 25755})],\n",
       " 'answer': 'We use a Likelihood to Buy model, which is a behavioral model that predicts the level of engagement of a person or account.'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We use a Likelihood to Buy model, which is a behavioral model that predicts the level of engagement of a person or account.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can you tell me what likelihood to buy is?'),\n",
       "  AIMessage(content='Likelihood to Buy: refers to the output of a behavioral model predicting the level of engagement of a person or account ')],\n",
       " 'input': 'how is it related to MQA?',\n",
       " 'context': [Document(page_content='Likelihood to Buy: refers to the output of a behavioral model predicting the level of engagement of a person or account. \\nLift (Conversion lift): impact on conversion compared to the average population. \\nA positive lift shows a positive impact on conversion of having this trait, or performing this action \\nA negative life shows a negative impact on conversion of having this trait or performing this action', metadata={'source': 'all.txt', 'start_index': 25988}),\n",
       "  Document(page_content='the Likelihood to Buy model (Behavioral)\\nhelps answers \"how are they using our product?\", \"what actions are most correlated to conversion?\", \"is now a good time to reach out?\"\\nThe third category of prediction models, the Lead Grade, is simply a combination of the two others.\\nThe Customer Fit model (demographic)', metadata={'source': 'all.txt', 'start_index': 15612}),\n",
       "  Document(page_content='MQA: model predicting the Likelihood to Buy of an Account. It shows the level of engagement of the account, based on behavioral and intent data. If MQA means Marketing Qualified Account, this acronym in the context of MadKudu is used as a proxy for any type of behavioral model at the Account level, whether it includes marketing activity or not.  \\nOpen Opp: Standard conversion definition of an Open Opportunity (probability = 0%). Configured in the Conversion mapping in the app.', metadata={'source': 'all.txt', 'start_index': 26755}),\n",
       "  Document(page_content='Likelihood to Buy: Looks at any interaction with your website, product, marketing team, sales team to identify patterns of your Leads and Accounts behaviors that lead to conversions. (What has this lead been doing recently?)\\nLead Grade: Combines the Customer Fit and Likelihood to Buy into one score to help you simplify your workflows\\nWe configure these models independently so if you’re not sure which ones are enabled in your account, reach out to your Customer Success Manager.\\n  F.A.Q', metadata={'source': 'all.txt', 'start_index': 6093})],\n",
       " 'answer': 'Likelihood to Buy is related to MQA as MQA is a model predicting the Likelihood to Buy of an Account, showing the level of engagement based on behavioral and intent data. MQA serves as a proxy for any type of behavioral model at the Account level, whether it includes marketing activity or not.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_retrieval_chain.invoke({\n",
    "    'chat_history': chat_history,\n",
    "    \"input\": \"how is it related to MQA?\"\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
